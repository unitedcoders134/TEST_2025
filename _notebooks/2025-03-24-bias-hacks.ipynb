{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "---\n",
    "layout: post\n",
    "title:  Bias\n",
    "description:  Bias\n",
    "permalink: /bias/\n",
    "courses: { csp: {week 1} } \n",
    "comments: true\n",
    "sticky_rank: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack #1\n",
    "\n",
    "**Example: Facial Recognition Software Bias**\n",
    "\n",
    "One well-known example of computer bias is facial recognition software. Studies have shown that some facial recognition systems have a significantly higher error rate when identifying people with darker skin tones, especially Black women.\n",
    "\n",
    "**Who is affected?**\n",
    "People of color, particularly Black women, are more likely to be misidentified or not recognized at all by facial recognition systems. This can have serious consequences, such as wrongful arrests or exclusion from services that rely on facial recognition for access.\n",
    "\n",
    "**Potential cause of bias:**\n",
    "Many facial recognition models are trained on datasets that contain mostly lighter-skinned faces. Because the algorithm “sees” fewer examples of darker-skinned individuals, it struggles to accurately recognize them. This bias often stems from a lack of diversity in training data and insufficient testing on different racial and ethnic groups.\n",
    "\n",
    "**Solution:**\n",
    "Developers can improve accuracy by ensuring diverse and representative training datasets, testing systems on a wide range of users, and implementing fairness audits before deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack #2\n",
    "\n",
    "I once tried using a voice assistant to set a reminder while in a noisy environment, but it kept misunderstanding my request. Since my voice is on the deeper side, I’ve also noticed that it sometimes struggles more with recognizing what I say compared to when my mom or sister use it. This was frustrating because I had to repeat myself multiple times, and it made me feel like the technology wasn’t designed to work equally well for different voices. One way to improve this would be to train voice recognition models on a wider variety of voice pitches, accents, and background noise conditions to make them more inclusive for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popcorn Hack #3\n",
    "\n",
    "Bias could sneak into a fitness tracking app if it assumes all users have the same physical abilities, fitness levels, or goals. For example, if the app primarily recommends high-intensity workouts and step goals based on data from young, able-bodied individuals, it might not be useful—or even discouraging—for older users, people with disabilities, or those with chronic health conditions.\n",
    "\n",
    "To ensure fairness and inclusivity, the app could include:\n",
    "\n",
    "Customizable Goals: Allow users to set their own fitness targets based on their personal abilities and health conditions.\n",
    "\n",
    "Adaptive Recommendations: Offer a variety of workout options, including low-impact exercises, chair workouts, and guided stretching for those with mobility challenges.\n",
    "\n",
    "Diverse Metrics: Track progress in multiple ways—not just steps or calories burned, but also improvements in flexibility, balance, or consistency.\n",
    "\n",
    "Accessibility Features: Ensure voice navigation, larger text options, and compatibility with assistive devices to accommodate different needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Hack \n",
    "\n",
    "**Digital Tool: YouTube**\n",
    "**Identifying Potential Bias:**\n",
    "YouTube's recommendation algorithm often reinforces filter bubbles by suggesting content similar to what a user has previously watched. This can create an echo chamber, limiting exposure to diverse viewpoints. Additionally, content creators from marginalized groups sometimes struggle with visibility due to algorithmic prioritization of mainstream or advertiser-friendly content. Accessibility for different age groups and languages also varies—auto-generated captions can be inaccurate for non-English languages.\n",
    "\n",
    "**Analyzing the Cause:**\n",
    "The bias likely stems from:\n",
    "\n",
    "**Algorithm Design**: The AI prioritizes engagement, often pushing sensational or extreme content to keep users watching.\n",
    "\n",
    "**Data Collection**: YouTube tracks watch history, likes, and clicks, reinforcing users' existing preferences rather than introducing diverse content.\n",
    "\n",
    "**Monetization Policies**: The demonetization of certain topics can disproportionately affect small creators or underrepresented voices.\n",
    "\n",
    "**Proposed Solution**:\n",
    "YouTube could implement a \"diversity mode\" option in recommendations, which introduces content from different perspectives and smaller creators. Additionally, improving transparency in how recommendations are made and ensuring diverse testing teams can help reduce biases in the algorithm."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
